<!DOCTYPE html>
<html lang="en">
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="description" content="其实不过是杂记">



<title>容器化实践记录 | 小记</title>



    <link rel="icon" href="/favicon.ico">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    



    
    
        
    


</head>
<body class="dark-theme">
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">小记</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/about">About</a>
                
                <input hidden id="switch_default" type="checkbox" class="switch_default">
                <label style="display:none" for="switch_default" class="toggleBtn"></label>
            </div>

        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">小记</a><a id="mobile-toggle-theme">·&nbsp;Dark</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/about">About</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
        <div class="main">
            <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">Expand all</a>
        <a onclick="go_top()">Back to top</a>
        <a onclick="go_bottom()">Go to bottom</a>
    </div>
</div>

<script>
    document.ready(
        function () {
            tocbot.init({
                tocSelector: '.tocbot-list',
                contentSelector: '.post-content',
                headingSelector: 'h1, h2, h3, h4, h5',
                collapseDepth: 1,
                orderedList: false,
                scrollSmooth: true,
            })
        }
    )

    function expand_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 6,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "collapse_toc()");
        b.innerHTML = "Collapse all"
    }

    function collapse_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 1,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "expand_toc()");
        b.innerHTML = "Expand all"
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">容器化实践记录</h1>
            
                <div class="post-meta">
                    
                        Author: <a itemprop="author" rel="author" href="/">Arthur Lu</a>
                    

                    
                        <span class="post-time">
                        Date: <a href="#">March 24, 2020&nbsp;&nbsp;22:53:15</a>
                        </span>
                    
                    
                </div>
            
        </header>

        <div class="post-content">
            <p>当前工作和个人实践中仍然有很多项目采用陈旧的部署方案，这无论是集成CI/CD，或是自动扩缩容都有极大的限制。</p>
<p>这里对传统项目转容器的一些个人实践记录，可能包含 Docker、k8s、Helm、Prometheus、istio 等相关的内容和组件。</p>
<h2 id="一、Docker-实践记录"><a href="#一、Docker-实践记录" class="headerlink" title="一、Docker 实践记录"></a>一、Docker 实践记录</h2><h3 id="1-1-安装部署"><a href="#1-1-安装部署" class="headerlink" title="1.1 安装部署"></a>1.1 安装部署</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 配置 yum 源</span></span><br><span class="line">$ yum-config-manager --add-repo https://mirrors.ustc.edu.cn/docker-ce/linux/centos/docker-ce.repo</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载必要依赖</span></span><br><span class="line">$ yum install -y yum-utils device-mapper-persistent-data lvm2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装 docker-ce</span></span><br><span class="line">$ yum install docker-ce</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动</span></span><br><span class="line">$ systemctl <span class="built_in">enable</span> docker</span><br><span class="line">$ systemctl start docker</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置指定用户的 sudo</span></span><br><span class="line">$ sudoedit /etc/sudoers</span><br><span class="line"><span class="variable">$&#123;username&#125;</span> ALL=(ALL)   ALL</span><br><span class="line"></span><br><span class="line"><span class="comment"># 允许某个用户执行docker命令不加sudo</span></span><br><span class="line">$ usermod -aG docker <span class="variable">$&#123;username&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置 docker 镜像源</span></span><br><span class="line">vim /etc/docker/daemon.json</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"registry-mirrors"</span>: [</span><br><span class="line">    <span class="string">"https://dockerhub.azk8s.cn"</span>,</span><br><span class="line">    <span class="string">"https://hub-mirror.c.163.com"</span></span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重启 docker</span></span><br><span class="line">$ systemctl daemon-reload</span><br><span class="line">$ systemctl restart docker</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭和卸载</span></span><br><span class="line">$ systemctl stop docker</span><br><span class="line">$ systemctl <span class="built_in">disable</span> docker</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看当前安装的docker组件</span></span><br><span class="line">$ yum list installed|grep docker</span><br><span class="line">$ yum remove <span class="variable">$&#123;yum-docker&#125;</span></span><br></pre></td></tr></table></figure>
<h3 id="1-2-Docker-和-systemd"><a href="#1-2-Docker-和-systemd" class="headerlink" title="1.2 Docker 和 systemd"></a>1.2 Docker 和 systemd</h3><p>我的 CentOS 7.x 版本的 Docker 是默认使用 systemd 进行控制的，包含三个部分：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">/usr/lib/systemd/system/docker.service</span><br><span class="line">/usr/lib/systemd/system/docker.socket</span><br><span class="line">/usr/lib/systemd/system/containerd.service</span><br></pre></td></tr></table></figure></p>
<p>dockerd 的配置文件是: <code>/etc/docker/daemon.json</code>，变更了dockerd配置文件后，通常需要使用 systemctl 进行 dockerd 重启：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ systemctl daemon-reload</span><br><span class="line">$ systemctl restart docker</span><br></pre></td></tr></table></figure></p>
<p>Docker 交由 sytemd 进行管理，所以使用 systemd 的命令集查询 Docker 日志即可：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看指定 systemd unit 的日志</span></span><br><span class="line">$ journalctl -u docker</span><br></pre></td></tr></table></figure></p>
<p>systemd 在 Linux 进行服务进程部署非常常见，包括 k8s 也是通过 systemd 进行管理的。systemd 背后是一系列的指令集，这里仅列出个人常用部分：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ hostnamectl                           <span class="comment"># 显示主机信息，包括查看 Linux 发行版本、以及当前的虚拟化方式。</span></span><br><span class="line">$ loginctl list-users                   <span class="comment"># 列出当前登录的用户</span></span><br><span class="line">$ loginctl show-user <span class="variable">$&#123;username&#125;</span>        <span class="comment"># 列出指定用户的信息</span></span><br><span class="line">$ systemd-analyze blame                 <span class="comment"># 查看服务启动耗时</span></span><br><span class="line">$ systemd <span class="built_in">enable</span> <span class="variable">$&#123;unit&#125;</span>                <span class="comment"># 将某个服务设置为开机启动，本质上就是在/etc/systemd/system/中配置超链接</span></span><br><span class="line">$ systemd <span class="built_in">disable</span> <span class="variable">$&#123;unit&#125;</span>               <span class="comment"># 删除超链接</span></span><br><span class="line">$ systemd list-unit-files               <span class="comment"># 列出注册 systemd 的所有内容</span></span><br><span class="line">$ systemd status <span class="variable">$&#123;unit&#125;</span>                <span class="comment"># 查看某个 unit 的状态</span></span><br><span class="line">$ systemctl daemon-reload               <span class="comment"># 重新加载所有修改过的配置文件</span></span><br><span class="line">$ systemctl start/stop/<span class="built_in">kill</span>/reload/restart <span class="variable">$&#123;unit&#125;</span>      <span class="comment"># reload 只是重新加载配置文件</span></span><br></pre></td></tr></table></figure></p>
<p>需要注意, enable 的 service 不一定是 activate，因为 enable 只是创建开机启动的超链接。当然，activate 的 service 不一定是 enable 的。</p>
<h3 id="1-3-容器的隔离"><a href="#1-3-容器的隔离" class="headerlink" title="1.3 容器的隔离"></a>1.3 容器的隔离</h3><p>容器是借由 namespace 技术进行资源隔离，这是直接由 Linux 系统提供的系统函数实现的，每个进程的资源都有所属的 namespace，且进程只能看到自己 namespace 下的资源。</p>
<p>这种隔离方案的优点是轻量，但是隔离的不够彻底，仍然有些资源是公用的，例如时间。</p>
<p>在资源的隔离上，需要注意进程对根路径的隔离。容器根路径的隔离是依托于chroot实现的，它可以改变进程的根路径。通常启动xx系统的容器，会将xx系统根路径上所依赖的文件，复制到一个新的路径上，进程在chroot到该路径上去，这样进程看到的就是xx系统根路径的文件。</p>
<p>可见，xx系统的容器，虽然资源上拥有xx系统的资源，但是内核层面仍然是用的宿主机内核（各个发行版本linux本身也是资源依赖不同导致，相同的内核linux可以装不同发行版本linux）。</p>
<p>docker exec 可以进入容器，这也是依赖于namespace的特性。当我们知道容器的进程id时，就可以查到它所有资源的namespace了：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将 container id 转为 进程pid</span></span><br><span class="line">$ docker inspect --format <span class="string">'&#123;&#123; .State.Pid &#125;&#125;'</span> 97b589f91f2a</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查 pid 下的 namespaces</span></span><br><span class="line">$ ll /proc/<span class="variable">$&#123;pid&#125;</span>/ns</span><br><span class="line">lrwxrwxrwx 1 root root 0 3月  27 01:18 ipc -&gt; ipc:[4026532348]</span><br><span class="line">lrwxrwxrwx 1 root root 0 3月  27 01:18 mnt -&gt; mnt:[4026532346]</span><br><span class="line">lrwxrwxrwx 1 root root 0 3月  27 01:18 net -&gt; net:[4026532351]</span><br><span class="line">lrwxrwxrwx 1 root root 0 3月  27 01:18 pid -&gt; pid:[4026532349]</span><br><span class="line">lrwxrwxrwx 1 root root 0 3月  27 01:18 user -&gt; user:[4026531837]</span><br><span class="line">lrwxrwxrwx 1 root root 0 3月  27 01:18 uts -&gt; uts:[4026532347]</span><br></pre></td></tr></table></figure></p>
<h3 id="1-4-容器的限制"><a href="#1-4-容器的限制" class="headerlink" title="1.4 容器的限制"></a>1.4 容器的限制</h3><p>容器是借由 cgroup 技术进行资源限制。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看挂载了哪些 cgroup</span></span><br><span class="line">$ mount -t cgroup</span><br><span class="line">cgroup on /sys/fs/cgroup/systemd <span class="built_in">type</span> cgroup (rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/usr/lib/systemd/systemd-cgroups-agent,name=systemd)</span><br><span class="line">cgroup on /sys/fs/cgroup/hugetlb <span class="built_in">type</span> cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb)</span><br><span class="line">cgroup on /sys/fs/cgroup/freezer <span class="built_in">type</span> cgroup (rw,nosuid,nodev,noexec,relatime,freezer)</span><br><span class="line">cgroup on /sys/fs/cgroup/blkio <span class="built_in">type</span> cgroup (rw,nosuid,nodev,noexec,relatime,blkio)</span><br><span class="line">cgroup on /sys/fs/cgroup/net_cls,net_prio <span class="built_in">type</span> cgroup (rw,nosuid,nodev,noexec,relatime,net_prio,net_cls)</span><br><span class="line">cgroup on /sys/fs/cgroup/perf_event <span class="built_in">type</span> cgroup (rw,nosuid,nodev,noexec,relatime,perf_event)</span><br><span class="line">cgroup on /sys/fs/cgroup/memory <span class="built_in">type</span> cgroup (rw,nosuid,nodev,noexec,relatime,memory)</span><br><span class="line">cgroup on /sys/fs/cgroup/pids <span class="built_in">type</span> cgroup (rw,nosuid,nodev,noexec,relatime,pids)</span><br><span class="line">cgroup on /sys/fs/cgroup/cpu,cpuacct <span class="built_in">type</span> cgroup (rw,nosuid,nodev,noexec,relatime,cpuacct,cpu)</span><br><span class="line">cgroup on /sys/fs/cgroup/cpuset <span class="built_in">type</span> cgroup (rw,nosuid,nodev,noexec,relatime,cpuset)</span><br><span class="line">cgroup on /sys/fs/cgroup/devices <span class="built_in">type</span> cgroup (rw,nosuid,nodev,noexec,relatime,devices)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在 /sys/fs/cgroup 目录下也可以看到当前哪些资源种类可以进行cgrou</span></span><br><span class="line">$ ls /sys/fs/cgroup</span><br><span class="line">drwxr-xr-x 6 root root  0 3月  27 2020 blkio</span><br><span class="line">lrwxrwxrwx 1 root root 11 3月  27 2020 cpu -&gt; cpu,cpuacct</span><br><span class="line">lrwxrwxrwx 1 root root 11 3月  27 2020 cpuacct -&gt; cpu,cpuacct</span><br><span class="line">drwxr-xr-x 6 root root  0 3月  27 2020 cpu,cpuacct</span><br><span class="line">drwxr-xr-x 4 root root  0 3月  27 2020 cpuset</span><br><span class="line">drwxr-xr-x 6 root root  0 3月  27 2020 devices</span><br><span class="line">drwxr-xr-x 4 root root  0 3月  27 2020 freezer</span><br><span class="line">drwxr-xr-x 4 root root  0 3月  27 2020 hugetlb</span><br><span class="line">drwxr-xr-x 6 root root  0 3月  27 2020 memory</span><br><span class="line">lrwxrwxrwx 1 root root 16 3月  27 2020 net_cls -&gt; net_cls,net_prio</span><br><span class="line">drwxr-xr-x 4 root root  0 3月  27 2020 net_cls,net_prio</span><br><span class="line">lrwxrwxrwx 1 root root 16 3月  27 2020 net_prio -&gt; net_cls,net_prio</span><br><span class="line">drwxr-xr-x 4 root root  0 3月  27 2020 perf_event</span><br><span class="line">drwxr-xr-x 6 root root  0 3月  27 2020 pids</span><br><span class="line">drwxr-xr-x 6 root root  0 3月  27 2020 systemd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 每个种类也叫子系统，在每个子系统中有资源限制文件。该子系统中的目录是控制组，创建一个目录的时候，就自动创建控制组，控制组下会生成所有的资源控制文件。</span></span><br><span class="line">$ <span class="built_in">cd</span> /sys/fs/cgroup/cpu &amp;&amp; ll</span><br><span class="line">-rw-r--r--   1 root root 0 3月  27 2020 cgroup.clone_children</span><br><span class="line">--w--w--w-   1 root root 0 3月  27 2020 cgroup.event_control</span><br><span class="line">-rw-r--r--   1 root root 0 3月  27 2020 cgroup.procs</span><br><span class="line">-r--r--r--   1 root root 0 3月  27 2020 cgroup.sane_behavior</span><br><span class="line">-r--r--r--   1 root root 0 3月  27 2020 cpuacct.stat</span><br><span class="line">-rw-r--r--   1 root root 0 3月  27 2020 cpuacct.usage</span><br><span class="line">-r--r--r--   1 root root 0 3月  27 2020 cpuacct.usage_percpu</span><br><span class="line">-rw-r--r--   1 root root 0 3月  27 2020 cpu.cfs_period_us</span><br><span class="line">-rw-r--r--   1 root root 0 3月  27 2020 cpu.cfs_quota_us</span><br><span class="line">-rw-r--r--   1 root root 0 3月  27 2020 cpu.rt_period_us</span><br><span class="line">-rw-r--r--   1 root root 0 3月  27 2020 cpu.rt_runtime_us</span><br><span class="line">-rw-r--r--   1 root root 0 3月  27 2020 cpu.shares</span><br><span class="line">-r--r--r--   1 root root 0 3月  27 2020 cpu.stat</span><br><span class="line">drwxr-xr-x   3 root root 0 3月  27 00:06 docker                 <span class="comment"># 我的docker控制组</span></span><br><span class="line">drwxr-xr-x   5 root root 0 3月  26 22:42 kubepods</span><br><span class="line">-rw-r--r--   1 root root 0 3月  27 2020 notify_on_release</span><br><span class="line">-rw-r--r--   1 root root 0 3月  27 2020 release_agent</span><br><span class="line">drwxr-xr-x 102 root root 0 3月  27 00:07 system.slice</span><br><span class="line">-rw-r--r--   1 root root 0 3月  27 2020 tasks</span><br><span class="line">drwxr-xr-x   2 root root 0 3月  26 22:42 user.slice</span><br><span class="line"></span><br><span class="line"><span class="comment"># 控制组中的tasks标示着当前有哪些进程处于该控制组，该控制组的资源受控制组下的。</span></span><br><span class="line"><span class="comment"># 需要注意，控制组下还可以有子控制组，其资源是在父控制组上进行限制的。</span></span><br></pre></td></tr></table></figure>
<p>每个docker容器都会生成一个控制组，控制组默认为 <code>/sys/fs/cgroup/${resource}/docker/${container_id}/</code>，因此容器的资源受docker控制组的限制，容器中的进程又受容器控制组限制。</p>
<h3 id="1-5-镜像的分层解构"><a href="#1-5-镜像的分层解构" class="headerlink" title="1.5 镜像的分层解构"></a>1.5 镜像的分层解构</h3><p>Docker 是使用 UnionFS 来组织文件的，该文件系统是一种分层解构。</p>
<p>可以通过 <code>docker image inspect ${image}</code> 来查看容器的一些信息，包括每个层的hash值。</p>
<p>Docker 会将镜像的层全部挂载在一个目录上，使用该镜像实例化容器的时候，会将这些层又挂载到一个容器的挂载目录下，容器操作文件也是通过的 UnionFS，且删除文件的时候不会真正的删除这些层中的文件，而是被遮挡起来。</p>
<p>有一点需要注意，容器运行时存在三层：</p>
<ul>
<li>第一层，可读写层，是容器进程对磁盘的变更均反映在该层。</li>
<li>第二层，初始化层，是docker的一些初始化工作。</li>
<li>第三层，只读层，是运行的镜像层。</li>
</ul>
<p>docker commit 只会打包第一层和第三层，第二层 docker 自身引入的一些变更不会打包。</p>
<h3 id="1-6-Dockerfile"><a href="#1-6-Dockerfile" class="headerlink" title="1.6 Dockerfile"></a>1.6 Dockerfile</h3><h4 id="1-6-1-ENTRYPOINT-和-CMD"><a href="#1-6-1-ENTRYPOINT-和-CMD" class="headerlink" title="1.6.1 ENTRYPOINT 和 CMD"></a>1.6.1 ENTRYPOINT 和 CMD</h4><p>在编写 Dockerfile 时，需要特别注意 ENTRYPOINT 以及 CMD:</p>
<ul>
<li>在 docker run 时，默认执行的命令脚本为: <code>ENTRYPOINT CMD</code>。</li>
<li>ENTRYPOINT 默认为 <code>/bin/sh -c</code>。</li>
<li>在 docker run 时，我们可以改写 CMD， 但是不能改写 ENTRYPOINT。</li>
</ul>
<h3 id="1-7-容器网络"><a href="#1-7-容器网络" class="headerlink" title="1.7 容器网络"></a>1.7 容器网络</h3><p>只会介绍 bridge 网络模式，因为 k8s 很多网络模型均是基于该模型的。</p>
<p>通常容器网络会设置 namespace 进行隔离，单个宿主机为了打通不同 namespace 下的容器网络，需要借助两个关键的虚拟网络设备:</p>
<ul>
<li>Bridge, docker服务启动后，会默认创建 docker0 网桥。</li>
<li>Veth Pair, 创建后会生成两个虚拟网卡，这两个网卡之间的数据是相通的，用来连接不同 namespace 下的网络。</li>
</ul>
<p>通常情况下，通过 Veth Pair, 将一端作为容器的 eth0 网卡，另一端插在宿主机的 docker0 网桥上，宿主机和容器的网络就被打通了，且宿主机成为了容器的交换机。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装 bridge 相关工具</span></span><br><span class="line">$ yum install -y bridge-utils</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看网桥信息</span></span><br><span class="line">$ brctl show</span><br><span class="line">bridge name	         bridge id		     STP enabled	   interfaces</span><br><span class="line">docker0		        8000.02427e3c0e47	       no		      veth1086589</span><br><span class="line">                                                      veth3cbb8f0</span><br><span class="line">                                                      vethf2a8737</span><br></pre></td></tr></table></figure>
<p>因为我开启了 三个容器 (Web Server &amp; Redis &amp; MySQL)，所以会有 三对 Veth Pair，在宿主机中的 Veth 均插在 docker0 的网桥上。</p>
<p>这非常明显，宿主机上启动的容器均被 docker0 将网络打通，处于一个局域网。docker0 作为交换机，通过 arp 可以拿到所有容器 eth0 网卡的 mac 地址，这样一个容器的 eth0 直接给另一个容器的 eth0 发送数据就能做到了。</p>
<p>Docker 默认的网络方案只适合单机容器之间进行通信，对于跨主机的容器通信则无法实现，但是 k8s 却是一个集群，不同节点上的容器可能存在相互依赖关系，如何实现跨主通信是 k8s 必须解决的问题，这在 k8s 网络部分会进行简述。</p>
<h2 id="二、Kubernetes-实践记录"><a href="#二、Kubernetes-实践记录" class="headerlink" title="二、Kubernetes 实践记录"></a>二、Kubernetes 实践记录</h2><h3 id="2-1-安装和部署"><a href="#2-1-安装和部署" class="headerlink" title="2.1 安装和部署"></a>2.1 安装和部署</h3><p>k8s 的安装可以借助 kubeadm 进行简化。</p>
<h4 id="2-1-1-master-配置"><a href="#2-1-1-master-配置" class="headerlink" title="2.1.1 master 配置"></a>2.1.1 master 配置</h4><p>环境配置<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 禁用防火墙</span></span><br><span class="line">$ systemctl stop firewalld</span><br><span class="line">$ systemctl <span class="built_in">disable</span> firewalld</span><br><span class="line"></span><br><span class="line"><span class="comment"># 禁用SELINUX</span></span><br><span class="line">$ setenforce 0</span><br><span class="line">$ cat /etc/selinux/config</span><br><span class="line">SELINUX=disabled</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置 k8s.conf</span></span><br><span class="line">$ vim /etc/sysctl.d/k8s.conf</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables = 1</span><br><span class="line">net.ipv4.ip_forward = 1</span><br><span class="line">$ modprobe br_netfilter</span><br><span class="line">$ sysctl -p /etc/sysctl.d/k8s.conf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置安装 k8s 的 yum 源</span></span><br><span class="line">$ cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo</span><br><span class="line">[kubernetes]</span><br><span class="line">name=Kubernetes</span><br><span class="line">baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=0</span><br><span class="line">repo_gpgcheck=0</span><br><span class="line">gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg</span><br><span class="line">        http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">$ yum makecache fast</span><br></pre></td></tr></table></figure></p>
<p>预先拉取 master 节点上涉及到的镜像:<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">docker pull cnych/kube-proxy-amd64:v1.10.0</span><br><span class="line">docker pull cnych/flannel:v0.10.0-amd64</span><br><span class="line">docker pull cnych/pause-amd64:3.1</span><br><span class="line">docker pull cnych/kubernetes-dashboard-amd64:v1.8.3</span><br><span class="line">docker pull cnych/heapster-influxdb-amd64:v1.3.3</span><br><span class="line">docker pull cnych/heapster-grafana-amd64:v4.4.3</span><br><span class="line">docker pull cnych/heapster-amd64:v1.4.2</span><br><span class="line">docker pull cnych/k8s-dns-kube-dns-amd64:1.14.8</span><br><span class="line">docker pull cnych/k8s-dns-dnsmasq-nanny-amd64:1.14.8</span><br><span class="line">docker pull cnych/k8s-dns-sidecar-amd64:1.14.8</span><br><span class="line"></span><br><span class="line">docker tag cnych/flannel:v0.10.0-amd64 quay.io/coreos/flannel:v0.10.0-amd64</span><br><span class="line">docker tag cnych/pause-amd64:3.1 k8s.gcr.io/pause-amd64:3.1</span><br><span class="line">docker tag cnych/kube-proxy-amd64:v1.10.0 k8s.gcr.io/kube-proxy-amd64:v1.10.0</span><br><span class="line"></span><br><span class="line">docker tag cnych/k8s-dns-kube-dns-amd64:1.14.8 k8s.gcr.io/k8s-dns-kube-dns-amd64:1.14.8</span><br><span class="line">docker tag cnych/k8s-dns-dnsmasq-nanny-amd64:1.14.8 k8s.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.14.8</span><br><span class="line">docker tag cnych/k8s-dns-sidecar-amd64:1.14.8 k8s.gcr.io/k8s-dns-sidecar-amd64:1.14.8</span><br><span class="line"></span><br><span class="line">docker tag cnych/kubernetes-dashboard-amd64:v1.8.3 k8s.gcr.io/kubernetes-dashboard-amd64:v1.8.3</span><br><span class="line">docker tag cnych/heapster-influxdb-amd64:v1.3.3 k8s.gcr.io/heapster-influxdb-amd64:v1.3.3</span><br><span class="line">docker tag cnych/heapster-grafana-amd64:v4.4.3 k8s.gcr.io/heapster-grafana-amd64:v4.4.3</span><br><span class="line">docker tag cnych/heapster-amd64:v1.4.2 k8s.gcr.io/heapster-amd64:v1.4.2</span><br></pre></td></tr></table></figure></p>
<p>现在可以正式进行 k8s 的启动了：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装 k8s</span></span><br><span class="line">$ yum install -y kubelet-1.10.0-0 kubectl-1.10.0-0 kubeadm-1.10.0-0</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改 kubeadm 配置文件中的参数</span></span><br><span class="line">$ vim /etc/systemd/system/kubelet.service.d/10-kubeadm.conf</span><br><span class="line">Environment=<span class="string">"KUBELET_CGROUP_ARGS=--cgroup-driver=cgroupfs"</span></span><br><span class="line">$ systemctl daemon-reload</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化 k8s, 且指定 k8s 版本，指定 k8s 集群的子网，指定 master 节点 ip</span></span><br><span class="line">$ kubeadm init --kubernetes-version=v1.10.0 --pod-network-cidr=10.244.0.0/16 --apiserver-advertise-address=192.168.0.87</span><br><span class="line"></span><br><span class="line"><span class="comment"># 记录下输出的 join 命令，用于将 worker 节点中运行，添加到 k8s 集群</span></span><br><span class="line"><span class="comment"># kubeadm join 192.168.0.87:6443 --token dcnc52.dur0hn8ydjgrjkbn --discovery-token-ca-cert-hash sha256:c65c7fde46465f08d426d37f0e1c3ebc6354e1a4fda56d426900ffdfe768929c</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 这个时候可以看到 k8s 已经处于 activate 状态了</span></span><br><span class="line">$ systemctl status kubelet</span><br><span class="line"></span><br><span class="line"><span class="comment"># 此时集群中的 nodes 仅仅只有一个master节点, 且并不处于 ready 状态，这是因为还没有配置网络插件</span></span><br><span class="line">$ kubectl get nodes</span><br><span class="line">NAME                      STATUS     ROLES     AGE       VERSION</span><br><span class="line">iz8vbcrus31oj4ht48k0u3z   NotReady   master    2m        v1.10.0</span><br></pre></td></tr></table></figure></p>
<h4 id="2-3-2-worker-配置"><a href="#2-3-2-worker-配置" class="headerlink" title="2.3.2 worker 配置"></a>2.3.2 worker 配置</h4><p>可以将一个节点加入集群，成为 worker:<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用和 master 相同的方法，安装好 kubeadm</span></span><br><span class="line">$ kubeadm join 192.168.0.87:6443 --token dcnc52.dur0hn8ydjgrjkbn --discovery-token-ca-cert-hash sha256:c65c7fde46465f08d426d37f0e1c3ebc6354e1a4fda56d426900ffdfe768929c</span><br></pre></td></tr></table></figure></p>
<h4 id="2-3-3-网络配置"><a href="#2-3-3-网络配置" class="headerlink" title="2.3.3 网络配置"></a>2.3.3 网络配置</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在没有配置好网络的情况下，节点均是 NotReady 状态</span></span><br><span class="line">$ kubectl get nodes</span><br><span class="line">NAME                      STATUS     ROLES     AGE       VERSION</span><br><span class="line">iz8vbcrus31oj4ht48k0u3z   NotReady   master    6m        v1.10.0</span><br><span class="line">iz8vbcrus31oj4ht48k0u4z   NotReady   &lt;none&gt;    18s       v1.10.0</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取 flannel 的 k8s 组件，并直接加载 pods, 这些 pods 均在 kube-system 的 ns 下</span></span><br><span class="line">$ wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml</span><br><span class="line">$ kubectl apply -f kube-flannel.yml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看 pods 状态</span></span><br><span class="line">$ kubectl get pods -n kube-system</span><br><span class="line">NAME                                              READY     STATUS    RESTARTS   AGE</span><br><span class="line">etcd-iz8vbcrus31oj4ht48k0u3z                      1/1       Running   0          10m</span><br><span class="line">kube-apiserver-iz8vbcrus31oj4ht48k0u3z            1/1       Running   0          10m</span><br><span class="line">kube-controller-manager-iz8vbcrus31oj4ht48k0u3z   1/1       Running   0          10m</span><br><span class="line">kube-dns-86f4d74b45-cqf86                         3/3       Running   0          11m</span><br><span class="line">kube-flannel-ds-amd64-9q4jx                       1/1       Running   0          1m</span><br><span class="line">kube-flannel-ds-amd64-r4f54                       1/1       Running   0          1m</span><br><span class="line">kube-proxy-85gmw                                  1/1       Running   0          11m</span><br><span class="line">kube-proxy-wlm4f                                  1/1       Running   0          5m</span><br><span class="line">kube-scheduler-iz8vbcrus31oj4ht48k0u3z            1/1       Running   0          10m</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重新查看 nodes 情况, 全部就绪</span></span><br><span class="line">$ kubectl get nodes</span><br><span class="line">NAME                      STATUS    ROLES     AGE       VERSION</span><br><span class="line">iz8vbcrus31oj4ht48k0u3z   Ready     master    11m       v1.10.0</span><br><span class="line">iz8vbcrus31oj4ht48k0u4z   Ready     &lt;none&gt;    5m        v1.10.0</span><br></pre></td></tr></table></figure>
<p>如果部署过程中有问题，可以重置 k8s: <code>$ kubeadm reset</code></p>
<h3 id="2-3-污点机制"><a href="#2-3-污点机制" class="headerlink" title="2.3 污点机制"></a>2.3 污点机制</h3><p>通常 k8s 需要两台节点才能进行配置，那是因为 master 节点默认是不能启动 Pod 的，这个限制是受 k8s 的污点机制影响。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 打上污点标签</span></span><br><span class="line">kubectl taint nodes <span class="variable">$&#123;node-name&#125;</span> foo=bar:NoSchedule</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看节点节点的污点标签，下面是一个 master 节点的污点标签（自动生成的）</span></span><br><span class="line">$ kubectl describe node <span class="variable">$&#123;node-names&#125;</span> | grep Taints</span><br><span class="line">Taints:             node-role.kubernetes.io/master:NoSchedule</span><br></pre></td></tr></table></figure>
<ul>
<li><code>${node-names}</code> 可以通过 <code>kubectl get nodes</code> 进行查看。</li>
<li>污点标签的格式是: <code>${key}=${value}:${effect}</code><ul>
<li>key-value, 是标准的kv对。</li>
<li>effect 是标签的影响，NoSchedule 代表不会部署 pods。可见 mater 默认标签为:<ul>
<li>key: <code>node-role.kubernetes.io</code></li>
<li>value: 空</li>
<li>effect: <code>NoSchedule</code></li>
</ul>
</li>
</ul>
</li>
</ul>
<p>如果希望标签在某个污点标签的节点上运行，需要配置<code>tolerations</code>：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span></span><br><span class="line"><span class="attr">  v1kind:</span> <span class="string">Pod</span></span><br><span class="line">  <span class="string">...</span></span><br><span class="line"><span class="attr">  spec:</span></span><br><span class="line"><span class="attr">    tolerations:</span></span><br><span class="line"><span class="attr">      - key:</span> <span class="string">"foo"</span></span><br><span class="line"><span class="attr">        operator:</span> <span class="string">"Exists"</span></span><br><span class="line"><span class="attr">        effect:</span> <span class="string">"NoSchedule"</span></span><br></pre></td></tr></table></figure></p>
<p>如果我们希望在单机进行 k8s 的测试，则需要将 master 节点的污点去除。<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl taint nodes --all node-role.kubernetes.io/master-</span><br></pre></td></tr></table></figure></p>
<h3 id="2-4-常用-API-对象"><a href="#2-4-常用-API-对象" class="headerlink" title="2.4 常用 API 对象"></a>2.4 常用 API 对象</h3><h4 id="2-4-1-Pods"><a href="#2-4-1-Pods" class="headerlink" title="2.4.1 Pods"></a>2.4.1 Pods</h4><p>pod 是一堆容器的集合，也是 k8s 的最小调度单位。通常这些容器之间是有依赖关系的，这些容器之间会共享网络、存储等相关资源（处于相同的 namespace 下）。</p>
<p>pod 运行在什么节点上完全上由 k8s 进行调度，也可以通过配置，将 pod 固定当某些 node 上。<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">test-flask</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    app:</span> <span class="string">web</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="comment"># nodeSelector:                   # 该配置可以让 pod 只在 disktype:ssd 的标签 node 上运行。</span></span><br><span class="line">  <span class="comment">#   disktype: ssd</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">    - name:</span> <span class="string">flaskapp-demo</span></span><br><span class="line"><span class="attr">      image:</span> <span class="string">jcdemo/flaskapp</span></span><br><span class="line"><span class="attr">      ports:</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">5000</span>       <span class="comment"># 容器内部的服务端口</span></span><br><span class="line">          <span class="comment"># hostPort: 5001          # 该配置可以将主机 5001 端口接收到的请求转发给容器 5000 端口</span></span><br></pre></td></tr></table></figure></p>
<p>相关操作命令:<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获得所有的 pods, 若没有指定 namespace 则查询的是 default 下的 pod</span></span><br><span class="line">$ kubectl get pods -n <span class="variable">$&#123;namespace&#125;</span> [--show-labels]</span><br><span class="line">NAME                  READY     STATUS    RESTARTS   AGE</span><br><span class="line">static-web            1/1       Running   0          55m</span><br><span class="line">tesk-flaks-rc-6ptr9   1/1       Running   0          1m</span><br><span class="line">tesk-flaks-rc-ztqjk   1/1       Running   0          1m</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查询 pod 的详细描述，包括ip、port、日志等等</span></span><br><span class="line">$ kubectl describe pods <span class="variable">$&#123;pod-name&#125;</span> -n <span class="variable">$&#123;namespace&#125;</span></span><br></pre></td></tr></table></figure></p>
<h4 id="2-4-2-RC-RS"><a href="#2-4-2-RC-RS" class="headerlink" title="2.4.2 RC/RS"></a>2.4.2 RC/RS</h4><p>RC(Replication Controller) 和 RS(Replica Set) 都可以用来管理 Pod 的副本个数，目的是 Pod 异常时可以自动重启新的 Pod，并且也方便在 Pod 处理能力不足时新增 Pod 副本数量。RS 是 RC 的进化版，功能完全一致，但是其 selector 拥有更强的选择能力。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ReplicationController</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">tesk-flaks-rc</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    name:</span> <span class="string">rc</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">3</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    name:</span> <span class="string">test-flask</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        name:</span> <span class="string">test-flask</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">flaskapp-demo</span></span><br><span class="line"><span class="attr">          image:</span> <span class="string">jcdemo/flaskapp</span></span><br><span class="line"><span class="attr">          ports:</span></span><br><span class="line"><span class="attr">            - containerPort:</span> <span class="number">5000</span>       <span class="comment"># 容器内部的服务端口</span></span><br></pre></td></tr></table></figure>
<p>注意：</p>
<ul>
<li>RC 的 selector 并不能选择已经创建好的 Pod，需要控制的 Pod 的信息必须在 spec.template 中给出。</li>
<li>pod 的名称是 tesk-flaks-rc-${随机后缀}</li>
</ul>
<p>相关操作命令:<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get rc -n <span class="variable">$&#123;namespace&#125;</span></span><br><span class="line">NAME            DESIRED   CURRENT   READY     AGE</span><br><span class="line">tesk-flaks-rc   2         2         2         12m</span><br><span class="line"></span><br><span class="line">$ kubectl describe rc tesk-flaks-rc</span><br></pre></td></tr></table></figure></p>
<h4 id="2-4-2-Deployment"><a href="#2-4-2-Deployment" class="headerlink" title="2.4.2 Deployment"></a>2.4.2 Deployment</h4><p>Deployment 管理 RC/RS ，而 RC/RS 管理 Pods。因此 Deployment 拥有 RS 的全部功能，除此之外，还具有以下能力：</p>
<ul>
<li>版本记录</li>
<li>回滚到指定版本</li>
<li>升级暂停</li>
<li>制定升级策略</li>
</ul>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">tesk-flask-deploy</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    name:</span> <span class="string">deploy</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">2</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">      name:</span> <span class="string">test-flask</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        name:</span> <span class="string">test-flask</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">flaskapp-demo</span></span><br><span class="line"><span class="attr">          image:</span> <span class="string">jcdemo/flaskapp</span></span><br><span class="line"><span class="attr">          ports:</span></span><br><span class="line"><span class="attr">            - containerPort:</span> <span class="number">5000</span>       <span class="comment"># 容器内部的服务端口</span></span><br></pre></td></tr></table></figure>
<p>相关操作命令:<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get deployment -n <span class="variable">$&#123;namespace&#125;</span></span><br><span class="line">NAME                DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">tesk-flask-deploy   2         2         2            2           2m</span><br><span class="line"></span><br><span class="line">$ kubectl describe deployment tesk-flask-deploy</span><br><span class="line"></span><br><span class="line"><span class="comment"># deployment 会自动生成 rs</span></span><br><span class="line">$ kubectl get rs</span><br></pre></td></tr></table></figure></p>
<h4 id="2-4-3-Service"><a href="#2-4-3-Service" class="headerlink" title="2.4.3 Service"></a>2.4.3 Service</h4><p>Service 用于对满足条件的 pod 进行组合，对外提供服务（容器外以及集群外）。</p>
<ul>
<li><p>配置 targetPort，并自动分配 ClusterIP。该模式的 ClusterIP 只是暴露在集群内部，只有集群内可以访问。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">test-flask-service-1</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    name:</span> <span class="string">test-flask</span>           <span class="comment"># pod 的 label</span></span><br><span class="line"><span class="attr">  ports:</span></span><br><span class="line"><span class="attr">  - protocol:</span> <span class="string">TCP</span></span><br><span class="line"><span class="attr">    port:</span> <span class="number">5000</span>                          <span class="comment"># pods外，通过 clusterIP:port 访问</span></span><br><span class="line"><span class="attr">    targetPort:</span> <span class="number">5001</span>                  <span class="comment"># 访问 clusterIP:port, 会自动将请求发送给 podIP:targetPort, 具体用哪个 podIP，默认采用的轮训负载均衡模式。</span></span><br><span class="line"><span class="attr">    name:</span> <span class="string">myapp-http</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>配置 NodePort。集群外有其中一个 Node IP，便可以通过ip:nodeport访问。</p>
<ul>
<li>nodepord是自动生成的。</li>
<li>NodePort的模式其实就是在ClusterPort的基础上，生成了一个额外的NodePort。<ul>
<li>可以通过 nodeip:nodeport 进行访问。</li>
<li>可以用 clusterip:port 进行访问。</li>
<li>可以用 podip:targetport 进行访问。<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">test-flask-service-2</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    name:</span> <span class="string">test-flask</span></span><br><span class="line"><span class="attr">  type:</span> <span class="string">NodePort</span></span><br><span class="line"><span class="attr">  ports:</span></span><br><span class="line"><span class="attr">  - protocol:</span> <span class="string">TCP</span></span><br><span class="line"><span class="attr">    port:</span> <span class="number">5002</span></span><br><span class="line"><span class="attr">    targetPort:</span> <span class="number">5000</span></span><br><span class="line"><span class="attr">    name:</span> <span class="string">myapp-http</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>当 Service 部署好后，k8s 也会生成对应的域名，域名格式为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;pod-name&gt;.&lt;svc-name&gt;.&lt;namespace&gt;.svc.cluster.local</span><br></pre></td></tr></table></figure></p>
<p>当通过域名来访问时，分为了两种类型:</p>
<ul>
<li>headless service，域名可以解析出每个 Pod 的实际 IP 地址。</li>
<li>normal service，域名解析出 VIP，通过每个节点上的 iptables 将 VIP 的请求路由给 Pod。</li>
</ul>
<p>相关命令:<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Service 建立好后，可以用该命令查看 ClusterIP 和 NodePort，对于 ClusterIP 和 NodePort 自动生成的场景非常必要</span></span><br><span class="line">$ kubectl get services</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看 Service 代理的 Pod IP（Endpoints）</span></span><br></pre></td></tr></table></figure></p>
<h4 id="2-4-4-Job"><a href="#2-4-4-Job" class="headerlink" title="2.4.4 Job"></a>2.4.4 Job</h4><ul>
<li>Job</li>
<li>CronJob</li>
</ul>
<h4 id="2-4-5-StatefulSet"><a href="#2-4-5-StatefulSet" class="headerlink" title="2.4.5 StatefulSet"></a>2.4.5 StatefulSet</h4><h3 id="2-5-k8s-存储结构"><a href="#2-5-k8s-存储结构" class="headerlink" title="2.5 k8s 存储结构"></a>2.5 k8s 存储结构</h3><h3 id="2-6-Flannel"><a href="#2-6-Flannel" class="headerlink" title="2.6 Flannel"></a>2.6 Flannel</h3><p>我个人使用的网络方案是 Flannel，Flannel支持三种实现：</p>
<ul>
<li>VXLAN。</li>
<li>host-gw。</li>
<li>UDP, 最早的方案，比较简单，但是性能差。</li>
</ul>
<h4 id="2-6-1-Flannel-UDP-方案"><a href="#2-6-1-Flannel-UDP-方案" class="headerlink" title="2.6.1 Flannel UDP 方案"></a>2.6.1 Flannel UDP 方案</h4><p>性能虽差，但是比较容易理解，记录一下。该方案之所以叫 UDP，可能是因为任何数据流通都被 fannel 封装为了 udp 包进转发。该方案中，互通节点均需要安装 flannel, 且均运行 <code>flanneld</code> 进程，这是 flannel 主进程，也是用于转发的进程。</p>
<p>flannel udp 方案的细节：</p>
<ul>
<li>flanneld 会监听本机 8285 端口。</li>
<li>flanneld 自己会维护一个子网表，该子网数据记录在 etcd 中，且通常每个 node 都会成为一个子网。</li>
<li>在 node 中虚拟化出一个 flannel0 的设备，这是一种 TUN 设备。该设备如果接收到了数据包，会直接发给 flanneld。</li>
<li>flanneld 会给每个宿主机分配一个子网ip，dockerd 启动时 配置 bip 为该ip即可，docker0 网桥就会以该 ip 作为交换机 ip。</li>
<li>flanneld 会配置路由表，将所有给其子网发送的数据包均路由给 flannel0 设备。</li>
<li>docker0 和 flanneld 之间会涉及内核态与用户态的转换。</li>
</ul>
<p>收发流程可以参考下图：<br><img src="flannel-udp.png" alt=""></p>
<p>频繁的内核与用户态的转换，有非常严重的性能问题。</p>
<h4 id="2-6-2-VxLAN"><a href="#2-6-2-VxLAN" class="headerlink" title="2.6.2 VxLAN"></a>2.6.2 VxLAN</h4><p>VxLAN 和 UDP 方案非常类似，但是可以在内核中进行封装、解封装、转发的操作。</p>
<p>flannel VxLAN 方案的细节：</p>
<ul>
<li>flannel 会在每个节点生成 flannel.1 设备，这是一个 VTEP 设备（VXLAN Tunnel End Point）。</li>
<li>当某个 node A 加入 flannel 网络时，会自动在所有 flannel 节点中配置路由表，将目的地为 node A 的 docker0 地址的请求，转发给 flannel.1 设备。</li>
<li>当某个 node A 加入 flannel 网络时，会自动将所有节点的 MAC 表进行更新，将 node A 的 flannel.1 设备的 MAC 地址进行注入，注入的实际数据记录了 node A 的 docker0 地址对应的 MAC 地址。（node A 的 docker0 的地址用 node A 的 VTEP MAC 地址进行表示）</li>
<li>当某个 node A 加入 flannel 网络时，会在 FDB 中进行记录，记录了某个 MAC 地址需要通过哪个IP进行转发。通常这里的MAC地址就是目标VTEP的MAC地址，而IP则是所在节点的 eth0 的IP。这是为了告诉 flannel.1 数据包该发送给哪个主机。</li>
<li>flannel.1 设备收到 docker0 传过来的数据包后，会进行 IP 包的封装，IP包的最外层使用<code>目的VTEP的MAC地址</code>以及<code>目的容器docker0的IP地址</code>。</li>
<li>flannel.1 在 IP 包的基础上继续封装，封装成 UDP 包，该包中发送给目标主机的 eth0。</li>
</ul>
<p>具体流程如下:<br><img src="flannel-vxlan.png" alt=""></p>
<h4 id="2-6-3-host-gw"><a href="#2-6-3-host-gw" class="headerlink" title="2.6.3 host-gw"></a>2.6.3 host-gw</h4><h3 id="2-7-k8s-网络模型"><a href="#2-7-k8s-网络模型" class="headerlink" title="2.7 k8s 网络模型"></a>2.7 k8s 网络模型</h3><p>k8s 没有使用 docker0 网桥，而是会给每个节点生成 cni0 作为虚拟网桥，其实功能和 docker0 完全一致。k8s 会为集群中的每个宿主机配置 cni0 以及生成对应的网段，而 cni0 的使用的网段需要在 fannel 的子网范围内。</p>
<p>在初始化 k8s 的时候，可以定义 fannel 管理的子网范围：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubeadm init --pod-network-cidr=10.244.0.0/16</span><br></pre></td></tr></table></figure></p>
<p>之前提到过，k8s 对于 pod 网络资源的共享，实际上会启动 Infra 容器，该容器作为初始容器，所需要的容器会加入 Infra 的 namespace。</p>
<p>之所以使用 cni0，这是因为 k8s 启动一个 pod 中的容器时，为了共享同一个 pod 中容器的网络资源，需要通过 CNI 插件（flannel 就是一种 CNI 插件）进行很多的设置。关于网络相关的操作，是通过 CRI（容器运行时接口）来进行的，这个被称为 dockershim。</p>
<p>k8s 的跨主机通信部分的流程完全符合 flannel 的 VxLAN 方案，只是使用的是 cni0 作为网桥。<br><img src="k8s-vxlan.png" alt=""></p>
<h2 id="三、Helm-实践记录"><a href="#三、Helm-实践记录" class="headerlink" title="三、Helm 实践记录"></a>三、Helm 实践记录</h2><h2 id="四、Prometheus-实践记录"><a href="#四、Prometheus-实践记录" class="headerlink" title="四、Prometheus 实践记录"></a>四、Prometheus 实践记录</h2><h3 id="4-1-安装部署"><a href="#4-1-安装部署" class="headerlink" title="4.1 安装部署"></a>4.1 安装部署</h3><h3 id="4-2-打通-Grafana"><a href="#4-2-打通-Grafana" class="headerlink" title="4.2 打通 Grafana"></a>4.2 打通 Grafana</h3><h2 id="五、Istio"><a href="#五、Istio" class="headerlink" title="五、Istio"></a>五、Istio</h2><h2 id="附录一、参考文献"><a href="#附录一、参考文献" class="headerlink" title="附录一、参考文献"></a>附录一、参考文献</h2><ul>
<li><a href="http://www.ruanyifeng.com/blog/2016/03/systemd-tutorial-part-two.html" target="_blank" rel="noopener">Systemd 入门教程：实战篇</a></li>
<li><a href="http://www.ruanyifeng.com/blog/2016/03/systemd-tutorial-part-two.html" target="_blank" rel="noopener">Systemd 入门教程：命令篇</a></li>
<li><a href="http://man7.org/linux/man-pages/man5/systemd.unit.5.html" target="_blank" rel="noopener">systemd.unit</a></li>
<li><a href="https://docs.docker.com/engine/docker-overview/" target="_blank" rel="noopener">Docker Overview</a></li>
<li><a href="https://docs.docker.com/engine/reference/builder/" target="_blank" rel="noopener">Dockerfile</a></li>
<li><a href="https://docs.docker.com/engine/reference/commandline/cli/" target="_blank" rel="noopener">Docker CLI(docker)</a></li>
<li><a href="https://docs.docker.com/engine/reference/commandline/dockerd/" target="_blank" rel="noopener">Docker CLI(dockerd)</a></li>
<li><a href="https://www.qikqiak.com/k8s-book/" target="_blank" rel="noopener">从Docker到Kubernetes</a></li>
</ul>
<h2 id="附录二、实践机的环境配置"><a href="#附录二、实践机的环境配置" class="headerlink" title="附录二、实践机的环境配置"></a>附录二、实践机的环境配置</h2><p>这个是个人喜好的配置。</p>
<ul>
<li><p>创建个人用户</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ useradd user00 -d /home/user00 -g users</span><br><span class="line">$ passwd user00</span><br></pre></td></tr></table></figure>
</li>
<li><p>配置 sudo</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudoedit /etc/sudoers</span><br><span class="line"></span><br><span class="line">user00 ALL=(ALL)   ALL</span><br></pre></td></tr></table></figure>
</li>
<li><p>配置 vim，可参考<a href="https://asirlu.com/2019/08/01/vim%E7%AC%94%E8%AE%B0" target="_blank" rel="noopener">vim笔记</a></p>
<ul>
<li><p>先配置插件管理组件:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -fLo ~/.vim/autoload/plug.vim --create-dirs https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim</span><br></pre></td></tr></table></figure>
</li>
<li><p>再配置.vimrc（至少得搞一个对目录树的支持）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">&quot; Plugin</span><br><span class="line">call plug#begin(&apos;~/.vim/plugged&apos;)</span><br><span class="line">Plug &apos;scrooloose/nerdtree&apos;</span><br><span class="line">call plug#end()</span><br><span class="line"></span><br><span class="line">syntax on</span><br><span class="line">set fileencodings=utf-8,ucs-bom,gb18030,gbk,gb2312,cp936</span><br><span class="line">set termencoding=utf-8</span><br><span class="line">set encoding=utf-8</span><br><span class="line">set termencoding=utf-8  </span><br><span class="line">set mouse=a</span><br><span class="line">set backspace=2</span><br><span class="line">set tabstop=4</span><br><span class="line">set expandtab</span><br><span class="line">set autoindent</span><br><span class="line">set smartindent</span><br><span class="line">set shiftwidth=4</span><br><span class="line">set number</span><br><span class="line"></span><br><span class="line">&quot; Map</span><br><span class="line">noremap &lt;space&gt;&lt;space&gt; viw</span><br><span class="line">noremap &lt;C-h&gt; &lt;C-w&gt;h</span><br><span class="line">noremap &lt;C-j&gt; &lt;C-w&gt;j</span><br><span class="line">noremap &lt;C-k&gt; &lt;C-w&gt;k</span><br><span class="line">noremap &lt;C-l&gt; &lt;C-w&gt;l</span><br><span class="line">noremap &lt;F1&gt; :NERDTreeFind&lt;CR&gt;</span><br><span class="line">noremap &lt;F2&gt; :NERDTreeToggle&lt;CR&gt;</span><br><span class="line">noremap &lt;F9&gt; :e ~/.vimrc&lt;CR&gt;</span><br><span class="line">noremap &lt;F10&gt; :source ~/.vimrc&lt;CR&gt;</span><br><span class="line"></span><br><span class="line">&quot; 激活</span><br><span class="line">&quot;:w -&gt; :source ~/.vimrc -&gt; :PlugInstall</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>

        </div>

        
            <section class="post-copyright">
                
                    <p class="copyright-item">
                        <span>Author:</span>
                        <span>Arthur Lu</span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>Permalink:</span>
                        <span><a href="http://github.com/lsj9383/2020/03/24/容器化实践记录/">http://github.com/lsj9383/2020/03/24/容器化实践记录/</a></span>
                    </p>
                
                <!-- 
                    <p class="copyright-item">
                        <span>License:</span>
                        <span>Copyright (c) 2019 <a href="http://creativecommons.org/licenses/by-nc/4.0/">CC-BY-NC-4.0</a> LICENSE</span>
                    </p>
                 -->
                <!-- 
                     <p class="copyright-item">
                         <span>Slogan:</span>
                         <span>Do you believe in <strong>DESTINY<strong>?</span>
                     </p>
                 -->

            </section>
        
        <!-- <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">back</a>
                <span>· </span>
                <a href="/">home</a>
            </div>
        </section> -->
        <section class="post-nav">
            
                <a class="prev" rel="prev" href="/2020/03/29/Tableau-Server-OIDC-AuthN/">Tableau Server OIDC AuthN</a>
            
            
            <a class="next" rel="next" href="/2020/03/15/生活点滴/">生活点滴</a>
            
        </section>


    </article>
</div>

        </div>
        <footer id="footer" class="footer">
    <div class="copyright">
        <span>© Arthur Lu | Powered by <a href="https://hexo.io" target="_blank">Hexo</a></span>
    </div>
</footer>

    </div><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>
